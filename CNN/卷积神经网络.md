# 卷积神经网络

卷积神经网络包括6个操作：**卷积、激活、池化、全连接层和反向传播、梯度下降**

## 1 卷积

### 1.1 卷积的定义

 我们称 ![[公式]](https://www.zhihu.com/equation?tex=%28f%2Ag%29%28n%29) 为 ![[公式]](https://www.zhihu.com/equation?tex=f%2Cg) 的卷积，卷积的本质是==降维==。

其连续的定义为：

![[公式]](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%28f%2Ag%29%28n%29%3D%5Cint+_%7B-%5Cinfty+%7D%5E%7B%5Cinfty+%7Df%28%5Ctau+%29g%28n-%5Ctau+%29d%5Ctau+%5C%5C)

其离散的定义为：

![[公式]](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%28f%2Ag%29%28n%29%3D%5Csum+_%7B%5Ctau+%3D-%5Cinfty+%7D%5E%7B%5Cinfty+%7D%7Bf%28%5Ctau+%29g%28n-%5Ctau+%29%7D%5C%5C)

其离散的定义为：

![[公式]](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%28f%2Ag%29%28n%29%3D%5Csum+_%7B%5Ctau+%3D-%5Cinfty+%7D%5E%7B%5Cinfty+%7D%7Bf%28%5Ctau+%29g%28n-%5Ctau+%29%7D%5C%5C)

这个特征有什么意义？

我们令 ![[公式]](https://www.zhihu.com/equation?tex=x%3D%5Ctau+%2Cy%3Dn-%5Ctau+) ，那么 ![[公式]](https://www.zhihu.com/equation?tex=x%2By%3Dn) 就是下面这些直线：

<img src="https://pic1.zhimg.com/50/v2-8be52f6bada3f7a21cebfc210d2e7ea0_hd.webp?source=1940ef5c" alt="img" style="zoom:50%;" />

如果遍历这些直线，就好比，把毛巾沿着角卷起来：

<img src="https://pic1.zhimg.com/50/v2-1d0c819fc7ca6f8da25435da070a2715_hd.jpg?source=1940ef5c" alt="img" style="zoom:50%;" />

### 1.2 离散卷积的例子：丢骰子

两枚骰子点数加在一起为4的概率是多少？

<img src="https://pic1.zhimg.com/v2-a67a711702ce48cd7632e783ae0a1f42_r.jpg?source=1940ef5c" alt="preview" style="zoom:50%;" />

因此，两枚骰子点数加起来为4的概率为：

![[公式]](https://www.zhihu.com/equation?tex=f%281%29g%283%29%2Bf%282%29g%282%29%2Bf%283%29g%281%29%5C%5C)

符合卷积的定义，把它写成标准的形式就是：

![[公式]](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%28f%2Ag%29%284%29%3D%5Csum+_%7Bm%3D1%7D%5E%7B3%7Df%284-m%29g%28m%29%5C%5C)

### 1.3 连续卷积的例子：做馒头

假设馒头的生产速度是 ![[公式]](https://www.zhihu.com/equation?tex=f%28t%29) ，那么一天后生产出来的馒头总量为：

![[公式]](https://www.zhihu.com/equation?tex=%5Cint+_%7B0%7D%5E%7B24%7Df%28t%29dt%5C%5C)

馒头生产出来之后，就会慢慢腐败，假设腐败函数为 ![[公式]](https://www.zhihu.com/equation?tex=g%28t%29) ，比如，10个馒头，24小时会腐败：

![[公式]](https://www.zhihu.com/equation?tex=10%2Ag%28t%29%5C%5C)

第一个小时生产出来的馒头，一天后会经历24小时的腐败，第二个小时生产出来的馒头，一天后会经历23小时的腐败。

如此，我们可以知道，一天后，馒头总共腐败了：

![[公式]](https://www.zhihu.com/equation?tex=%5Cint+_%7B0%7D%5E%7B24%7Df%28t%29g%2824-t%29dt%5C%5C)

这就是连续的卷积。

### 1.4 图像处理

用一个动图来说明下计算过程：

<img src="https://pic1.zhimg.com/50/v2-c658110eafe027eded16864fb6a28f46_hd.webp?source=1940ef5c" alt="img" style="zoom:80%;" />

这样相当于实现了 ![[公式]](https://www.zhihu.com/equation?tex=g) 这个矩阵在原来图像上的划动(准确来说，下面这幅图把 ![[公式]](https://www.zhihu.com/equation?tex=g) 矩阵旋转了 ![[公式]](https://www.zhihu.com/equation?tex=180%5E%5Ccirc) ）：

<img src="https://pic2.zhimg.com/50/v2-15fea61b768f7561648dbea164fcb75f_hd.webp?source=1940ef5c" alt="img" style="zoom: 50%;" />

<img src="C:\Users\49252\AppData\Roaming\Typora\typora-user-images\image-20201215133041928.png" alt="image-20201215133041928" style="zoom: 33%;" />

注意：==三维图像就是27个数相加==

**卷积操作的用处：**==用输出图像中更亮的像素表示原始图像中存在的边缘==

<img src="https://pic4.zhimg.com/80/v2-8cb00bf9fbf64322d6630b9cded95207_720w.jpg?source=1940ef5c" alt="img" style="zoom:50%;" />

### 1.5 卷积层

需要注意的是，**我们文献里的卷积其实不是卷积，而是互相关（cross—correlation），没有经过副对角线对称。 **

==每一个卷积核其实相当于对应一个特征图。==对于mnist手写中，当像素模块和卷积核一样的时候，那么得到的数值最大，所以说，相当于这个卷积核在滑动的时候，是在==找寻和他最像的那个像素模块。==

所以越多的卷积核可以提取越多的特征。

其中**局部连接和参数共享**是卷积神经网络最重要的两个性质。

#### 1.5.1 步长strides

* 步数越小，得到的特征就越精细；

* 一般stride = 1，因为当s=1时，可以很容易使特征图和原图像大小相等。

* ==有种说法是当stride大于1时，相当于池化操作，需要考证==

#### 1.5.2 0填充padding

设定原始图像大小为n∗n,卷积核大小为f∗f，则经过卷积操作后特征图大小为(n−f+1)∗(n−f+1)。

- 如果需要使经过卷积后的特征图大小保持不变，则填充大小需要满足公式

  n+2p−f+1=n

  即

  p=(f−1)/2

- **所以只要f即卷积核的边长是奇数**，则能保证输出的特征图大小与原图像大小相等。

- 甚至可以说，几乎所有的过滤器的边长都是奇数的！还有一点需要说明的是，**当有个中间点，便于索引到滤波器的位置。**

而不适用padding的缺点：

1. 像素会变小
2. 边缘角落的像素，只会触碰一次；而中间的像素会被扫描多次，因此感受野就会侧重中间位置的特征。

#### 1.5.3 特征图像素的计算

公式：==(n+2p-f)/s + 1== ；其中s=stride

## 2 激活层

### 2.1 常用激活函数

激活函数的三个特点：**一是可微，二是非线性，三是单调性**

#### 2.1.1 sigmoid函数

$$
f(x)=\frac{1}{1+e^{-x}}
$$



其求导结果是：
$$
f'(x) = a(1-a)
$$

<img src="https://upload-images.jianshu.io/upload_images/2164115-ed8922edfe9cc2a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png" style="zoom:50%;" />

特点：

1. 可以将值限定在（0，1）之间，只能用在二分类；
2. 缺点是当输入太大或太小时，导数就为0，才有梯度下降会很慢很慢。
3. sigmoid函数要进行指数运算，这个对于计算机来说是比较慢的。

#### 2.1.2 tanh 函数

$$
a = thah(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
$$

<img src="https://upload-images.jianshu.io/upload_images/2164115-7df91d4872a5cca6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png" style="zoom:50%;" />

特点：和sigmoid相近，但是好处在于它是关于零对称的。

一般二分类问题，隐含层用sigmoid函数，最后输出层用tanh函数

#### 2.1.3 Relu函数

$$
a = ReLU(z) = max(0,z)
$$

<img src="https://upload-images.jianshu.io/upload_images/2164115-53b342359897681b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image" style="zoom: 33%;" />

优点：

1) 在输入为正数的时候，不存在梯度饱和问题。

2) 计算速度要快很多。Relu函数只有线性关系，不管是前向传播还是反向传播，都比sigmoid和tanh要快很多。（sigmoid和tanh要计算指数，计算速度会比较慢）

当然，缺点也是有的：

1) 当输入是负数的时候，Relu是完全不被激活的，这就表明一旦输入到了负数，Relu就会死掉。这样在前向传播过程中，还不算什么问题，有的区域是敏感的，有的是不敏感的。但是到了反向传播过程中，输入负数，梯度就会完全到0，这个和sigmoid函数、tanh函数有一样的问题。

2) 我们发现Relu函数的输出要么是0，要么是正数，这也就是说，Relu函数也不是以0为中心的函数。

####  2.1.4 卷积中的激活函数

例如经过Relu激活函数的特征为：

<img src="https://pic4.zhimg.com/80/v2-f9af9fde70d5d3b7db5562956c6cc213_720w.png" alt="img" style="zoom: 50%;" />



## 3 池化

一般分为两种：max pooling和average pooling。

<img src="https://pic4.zhimg.com/v2-ac441205fd06dc037b3db2dbf05660f7_b.webp" alt="img" style="zoom:50%;" />

需要注意的是，池化后的特征保留了这一块最佳匹配的结果，==但不会具体关注哪部分匹配了，而只关注是不是某地方匹配了==

那么为什么使用池化呢？

**池化的本质是降维。当上一步卷积的stride =1时，如果t步检测到很大的强边缘，那么t+1步也可以检测较强的边缘特征。但是t+1的特征对于t步的特征来说，算得上是冗余信息。因此我们使用max pooling可以解决这个问题。 ** 

**而且可以增加提取特征的鲁棒性。**

## 4 全连接层

假若我们池化后的特征是3X3X5的特征块，那我们应该怎样去全连接呢？

我们可以把每一个特征块看做一个神经元，如果我们要用4096个神经元进行全连接的话，实际上我们需要的是3X3X5X4096的神经元。

也可以理解为==3X3X5X4096==的卷积核进行全局卷积。

<img src="https://pic1.zhimg.com/v2-677c85adc52245a0c3bd6c766e057da8_b.jpg" alt="img" style="zoom: 80%;" />

* 那么**全连接的意义**到底是什么呢？

其实他就是讲每一个卷积核提取的特征块整合在一起，输出了一个值。

那么这意味着我可以==大大减少特征位置对分类造成的影响。==

<img src="https://pic2.zhimg.com/v2-de4ba4bac6abed53025026f877fd80d1_b.jpg" alt="img" style="zoom:67%;" />

输出的值越大那么出现卷积核提取的这个特征的可能性越大，我不需要管他的位置。

* **那么为什么一般全连接都要两层呢？**

但是只用一层fully connected layer 有时候没法解决非线性问题

**而如果有两层或以上fully connected layer就可以很好地解决非线性问题了**

全连接层的作用是**分类**

<img src="https://pic2.zhimg.com/v2-671995a238e33a1c4e669340fed561f5_b.jpg" alt="img" style="zoom:80%;" />

因为我们可以具象的理解为我们得第一层全连接神经网络是==提取细节特征==，第二层是为了==整合特征==。

## 5 训练

### 5.1 代价函数

常用的是平方误差函数：

![卷积神经网络(CNN)学习笔记：模型训练](https://plobimage.ybzhao.com/wp-content/uploads/2018/04/2sdfsgzg.png)

常用的梯度下降：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Beqnarray%2A%7D+g%28%5Cbold%7Bx%7D%29%3D%5Cbegin%7Bcases%7D+%5Cbold%7Bx%7D+-+%5Ceta%5Cnabla+f%28%5Cbold%7Bx%7D%29+%26%5Ctext%7B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%7D%2C%5C%5C+%5Cbold%7Bx%7D+%2B+%5Ceta%5Cnabla+f%28%5Cbold%7Bx%7D%29+%26%5Ctext%7B%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87%E6%B3%95%7D.+%5Cend%7Bcases%7D+%5Cend%7Beqnarray%2A%7D)

### 5.2 反向传播

详情请点击[BP反向传播](D:/桌面/笔记/BP反向传播.md)

## 6 常用的神经网络框架

* Lenet-5
* AlexNet
* VGG -16
* [Resnet 残差网络](D:/桌面/笔记/CNN/残差网络（ResNet）.md)(Residual network)  2015年
* [Inception network](D:/桌面/笔记/CNN/Inception network.md)

## 7 知识拓展

### 7.1 边缘检测 

卷积神经网络其实就是算是==学习滤波器的参数。==因为经典的滤波器是固定参数的，有固定的目的，检测固定的边缘。当使用神经网络去学习这个参数的时候，我们就不仅仅可以检测到或垂直或水平等之类的边缘参数了。

#### 7.1.1 垂直边缘检测

垂直滤波器原理：

<img src="C:\Users\49252\AppData\Roaming\Typora\typora-user-images\image-20201215114338904.png" alt="image-20201215114338904" style="zoom:50%;" />

思考：为什么这个滤波器矩阵是检测垂直的呢？

**看上面中间的滤波器矩阵，左边是检测高光，右边是检测低光，中间的不需要考虑。因此当图像两边出现特别明显的灰度值差，就会被视作为垂直边缘。**

典型的垂直滤波器：

* sobel filter

<img src="C:\Users\49252\AppData\Roaming\Typora\typora-user-images\image-20201215115637728.png" alt="image-20201215115637728" style="zoom:67%;" />

<center style="color:#C0C0C0">sobel filter.jpg</center>

特点：增加了中间一排的权重，使提高了鲁棒性。

* Scharr滤波器

<img src="C:\Users\49252\AppData\Roaming\Typora\typora-user-images\image-20201215125456797.png" alt="image-20201215125456797" style="zoom:67%;" />

<center style="color:#C0C0C0">Scharr filter.jpg</center>

#### 7.1.2 水平边缘检测

**同上**

### 7.2 迁移学习

面向github编程（狗头

**使用条件：**

1. 目标任务和原任务相近
2. 数据集较小

**迁移学习最关键的一点是在完成原任务的基础上，重新设置一个较小的==学习率==，训练新的任务。**

### 7.3 数据扩充

**问题：数据较少时，可以使用此方法。**

1. 镜像翻转
2. 随机修剪
3. 旋转或扭曲等
4. 色彩转换、颜色扭曲、PCA颜色增强（假如R,B颜色深，G颜色浅的话，那么此算法就会将图像看起来更均衡一些）

### 7.4 10-crop

![image-20201222205700335](C:\Users\49252\AppData\Roaming\Typora\typora-user-images\image-20201222205700335.png)

选择十个图像位置然后分别用分类器进行分类，将分类结果进行平均，同时不仅仅只能十个。